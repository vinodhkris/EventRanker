from urllib import urlopen
import urllib2
import utils
import json
from bs4 import BeautifulSoup
from collections import defaultdict
def parse_nyt():
	
	url = "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=stock+market+crash+&begin_date=20070101&end_date=20090101&api-key=318a69b2af97848f66071cb4c1fdc831:15:69992102" 
	response = urlopen(url).read()
	response = json.loads(response)
	
	print "Got response from nytimes"

	articleContent = []
	i = 0
	page = 1
	hits = response["response"]["meta"]["hits"]
	while i<51 and page<(hits/10):
		print 'Getting response for page',page
		url = "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=stock+market+crash+&begin_date=20070101&end_date=20090101&page="+str(page)+"&api-key=318a69b2af97848f66071cb4c1fdc831:15:69992102" 
		response = urlopen(url).read()
		response = json.loads(response)
		for article in response["response"]["docs"]:
			if article["word_count"]>200 and article["lead_paragraph"]!=None:
				articleContent.append({})
				articleContent[i]["abstract"] = article["abstract"]
				articleContent[i]["pub_date"] = article["pub_date"]
				articleContent[i]["headline"] = article["headline"]["main"]
				articleContent[i]["keywords"] = article["keywords"]
				articleContent[i]["lead_paragraph"] = article["lead_paragraph"]
				articleContent[i]["web_url"] = article["web_url"]
				articleContent[i]["id"] = article["_id"]
				articleContent[i]["word_count"] = article["word_count"]
				i+=1
				print 'Extracted',i,article["word_count"]
				keywords = ""
                keywords = getMultiples(article["keywords"],"value")
                # should probably pull these if/else checks into a module
                variables = [article["pub_date"], keywords, str(article["headline"]["main"]).decode("utf8").replace("\n","") if "main" in article["headline"].keys() else "", str(article["source"]).decode("utf8") if "source" in article.keys() else "", str(article["document_type"]).decode("utf8") if "document_type" in article.keys() else "", article["web_url"] if "web_url" in article.keys() else "",str(article["news_desk"]).decode("utf8") if "news_desk" in article.keys() else "",str(article["section_name"]).decode("utf8") if "section_name" in article.keys() else "",str(article["snippet"]).decode("utf8").replace("\n","") if "snippet" in article.keys() else "",str(article["lead_paragraph"]).decode("utf8").replace("\n","") if "lead_paragraph" in article.keys() else ""]
                line = "\t".join(variables)
                articleContent[i]["text"] = line
				if i>51:
					break
		page+=1

	print "Articles Extracted",i
	
	for i in xrange(len(articleContent)):
		print i,articleContent[i]["headline"],articleContent[i]["lead_paragraph"],articleContent[i]["word_count"],articleContent[i]["text"]
		raw_input()

	return articleContent

parse_nyt()